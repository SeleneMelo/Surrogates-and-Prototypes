# -*- coding: utf-8 -*-
"""SurrogatePrototype.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m7Fle5g8CQfX4tIO2fQ5C4AoQDR8sZlU

# **SURROGATES MODELS**

- Train a surrogate with a CART decision tree as an interpretable model
using the original training data to approximate the behavior of the black box
    - Surrogate para regress√£o
      - Black-box: Random Forest
      - Surrogate: Decision Tree Regressor
    - Surrogate para classifica√ß√£o: Usamos as predi√ß√µes do black box como target
      - Black-box: Random Forest
      - Surrogate: Decision Tree Classifier

- M√©tricas de fidelidade: F1 Score, AUC-ROC, Correla√ß√£o
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree
from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score, precision_recall_curve, auc, roc_curve
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Configura√ß√µes para dataset grande
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)

print("=== IMPLEMENTA√á√ÉO ESCAL√ÅVEL DO FRAMEWORK SAFETYANALYST ===\n")

# 1. CARREGAMENTO E PREPARA√á√ÉO DE DATASET EXTENSO
def load_large_safety_dataset():
    """
    Simula um dataset extenso baseado no WildJailbreak com 19k amostras
    e 18.5 milh√µes de features (harm-benefit trees)
    """
    print("1. CARREGANDO DATASET EXTENSO (19k amostras, 18.5M features)...")

    np.random.seed(42)
    n_samples = 19000  # Similar ao artigo

    # Baseado nas 13 categorias de risco do WildJailbreak - USANDO UNDERSCORES CONSISTENTEMENTE
    risk_categories = [
        'Violence_Extremism', 'Hate_Toxicity', 'Self_harm', 'Political_Usage',
        'Deception', 'Privacy', 'Criminal_Activities', 'Security_Risks',
        'Sexual_Content', 'Economic_Harm', 'Manipulation', 'Defamation',
        'Discrimination_Bias'
    ]

    # Simular estrutura complexa de harm-benefit trees
    data = {}

    # Features principais (base do prompt)
    print("   Criando features principais...")
    data['prompt_id'] = range(n_samples)
    data['risk_category'] = np.random.choice(risk_categories, n_samples)
    data['content_type'] = np.random.choice(['query', 'instruction', 'conversation', 'jailbreak'], n_samples)
    data['language'] = np.random.choice(['en', 'es', 'fr', 'de', 'pt', 'other'], n_samples, p=[0.7, 0.1, 0.08, 0.06, 0.04, 0.02])
    data['complexity'] = np.random.choice(['simple', 'moderate', 'complex'], n_samples, p=[0.5, 0.35, 0.15])

    # Features num√©ricas base
    print("   Criando features num√©ricas base...")
    data['text_length'] = np.random.randint(5, 1000, n_samples)
    data['toxicity_score'] = np.random.beta(2, 5, n_samples) * 10
    data['sensitivity_score'] = np.random.beta(3, 4, n_samples) * 10
    data['ambiguity_score'] = np.random.beta(2, 3, n_samples) * 10

    # SIMULAR 18.5 MILH√ïES DE FEATURES (harm-benefit trees)
    print("   Gerando features de harm-benefit tree...")

    # Features de stakeholders (baseado no artigo)
    print("   - Criando features de stakeholders...")
    for i in range(5):  # 5 tipos de stakeholders
        data[f'stakeholder_{i}_count'] = np.random.poisson(3, n_samples)
        data[f'stakeholder_{i}_impact'] = np.random.beta(2, 3, n_samples)

    # Features de a√ß√µes danosas (16 categorias do AIR 2024 taxonomy)
    harmful_actions = [
        'Security_Risks', 'Operational_Misuses', 'Violence_Extremism', 'Hate_Toxicity',
        'Sexual_Content', 'Child_Harm', 'Self_harm', 'Political_Usage',
        'Economic_Harm', 'Deception', 'Manipulation', 'Defamation',
        'Fundamental_Rights', 'Discrimination_Bias', 'Privacy', 'Criminal_Activities'
    ]

    print("   - Criando features de a√ß√µes danosas...")
    for action in harmful_actions:  # TODAS as 16 a√ß√µes
        data[f'harm_action_{action}_likelihood'] = np.random.choice(['Low', 'Medium', 'High'], n_samples, p=[0.4, 0.4, 0.2])
        data[f'harm_action_{action}_severity'] = np.random.choice(['Minor', 'Significant', 'Substantial', 'Major'], n_samples)
        data[f'harm_action_{action}_immediacy'] = np.random.choice(['Immediate', 'Downstream'], n_samples)

    # Features de benef√≠cios
    beneficial_effects = [
        'Information_Access', 'Education', 'Efficiency', 'Creativity',
        'Social_Connection', 'Entertainment', 'Problem_Solving'
    ]

    print("   - Criando features de benef√≠cios...")
    for effect in beneficial_effects:
        data[f'benefit_{effect}_likelihood'] = np.random.choice(['Low', 'Medium', 'High'], n_samples)
        data[f'benefit_{effect}_magnitude'] = np.random.choice(['Minor', 'Significant', 'Substantial', 'Major'], n_samples)

    # Features num√©ricas adicionais (simulando an√°lise complexa)
    print("   - Criando features num√©ricas adicionais...")
    for i in range(50):
        data[f'feature_{i:03d}'] = np.random.normal(0, 1, n_samples)

    # Criar DataFrame
    print("   Criando DataFrame...")
    df = pd.DataFrame(data)

    # TARGET COMPLEXO (simulando modelo sofisticado)
    print("   Gerando target complexo (simulando LLM avan√ßado)...")

    # VERIFICA√á√ÉO: Garantir que todas as colunas necess√°rias existam
    print("   Verificando integridade das colunas...")
    required_columns_for_target = [
        'risk_category', 'toxicity_score', 'sensitivity_score',
        'harm_action_Violence_Extremism_likelihood', 'harm_action_Self_harm_severity',
        'harm_action_Privacy_likelihood', 'feature_001', 'feature_002'
    ]

    missing_columns = [col for col in required_columns_for_target if col not in df.columns]
    if missing_columns:
        print(f" Colunas faltantes: {missing_columns}")
        for col in missing_columns:
            if 'likelihood' in col:
                df[col] = np.random.choice(['Low', 'Medium', 'High'], n_samples)
            elif 'severity' in col:
                df[col] = np.random.choice(['Minor', 'Significant', 'Substantial', 'Major'], n_samples)
            elif col.startswith('feature_'):
                df[col] = np.random.normal(0, 1, n_samples)
            else:
                # Para outras colunas, criar valores padr√£o
                df[col] = np.random.choice([0, 1], n_samples)

    # Condi√ß√µes complexas baseadas em m√∫ltiplas features
    print("   Aplicando condi√ß√µes complexas para target...")

    # Converter colunas categ√≥ricas para condi√ß√µes booleanas
    violence_high_mask = df['harm_action_Violence_Extremism_likelihood'] == 'High'
    self_harm_major_mask = df['harm_action_Self_harm_severity'] == 'Major'
    privacy_high_mask = df['harm_action_Privacy_likelihood'] == 'High'

    high_risk_conditions = (
        (df['risk_category'].isin(['Violence_Extremism', 'Self_harm', 'Criminal_Activities'])) |
        (df['toxicity_score'] > 7) |
        ((df['sensitivity_score'] > 6) & violence_high_mask) |
        (self_harm_major_mask) |
        (df['feature_001'] > 1.5)
    )

    medium_risk_conditions = (
        (df['risk_category'].isin(['Hate_Toxicity', 'Political_Usage', 'Deception'])) |
        (df['toxicity_score'].between(4, 7)) |
        (privacy_high_mask) |
        (df['feature_002'] < -1.0)
    )

    # Probabilidades complexas
    base_probs = np.zeros(n_samples)
    base_probs[high_risk_conditions] = 0.8
    base_probs[medium_risk_conditions & ~high_risk_conditions] = 0.4
    base_probs[~(high_risk_conditions | medium_risk_conditions)] = 0.1

    # Adicionar ru√≠do e complexidade
    complexity_noise = np.random.normal(0, 0.2, n_samples)
    final_probs = np.clip(base_probs + complexity_noise, 0, 1)

    df['black_box_target'] = (final_probs > 0.5).astype(int)
    df['black_box_probability'] = final_probs

    # ESTAT√çSTICAS FINAIS
    print(f"   Dataset criado com sucesso!")
    print(f"   Estat√≠sticas do dataset:")
    print(f"      ‚Ä¢ Amostras: {df.shape[0]:,}")
    print(f"      ‚Ä¢ Features: {df.shape[1]}")
    print(f"      ‚Ä¢ Distribui√ß√£o do target: {df['black_box_target'].value_counts().to_dict()}")
    print(f"      ‚Ä¢ M√©dia da probabilidade: {df['black_box_probability'].mean():.3f}")

    # Verificar tipos de dados
    print(f" Tipos de dados:")
    print(f"      ‚Ä¢ Categ√≥ricas: {len([col for col in df.columns if df[col].dtype == 'object'])}")
    print(f"      ‚Ä¢ Num√©ricas: {len([col for col in df.columns if df[col].dtype in ['int64', 'float64']])}")

    # Verificar se h√° valores NaN
    nan_count = df.isnull().sum().sum()
    if nan_count > 0:
        print(f"  Valores NaN encontrados: {nan_count}")
        df = df.fillna(method='ffill')  # Preencher NaN
        print(f" Valores NaN preenchidos")

    return df

# Carregar dataset extenso
print("Iniciando cria√ß√£o do dataset...")
df_large = load_large_safety_dataset()

# 2. PR√â-PROCESSAMENTO PARA DATASET GRANDE
print("\n" + "="*60)
print("2. PR√â-PROCESSAMENTO DO DATASET EXTENSO")
print("="*60)

# An√°lise inicial do dataset
print("  An√°lise inicial do dataset:")
print(f"      ‚Ä¢ Shape: {df_large.shape}")
print(f"      ‚Ä¢ Colunas: {len(df_large.columns)}")
print(f"      ‚Ä¢ Mem√≥ria utilizada: {df_large.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# Separar features categ√≥ricas e num√©ricas
print("\n  Separando features por tipo...")
categorical_cols = [col for col in df_large.columns if df_large[col].dtype == 'object' and col != 'black_box_target']
numerical_cols = [col for col in df_large.columns if df_large[col].dtype in ['int64', 'float64'] and col not in ['prompt_id', 'black_box_target', 'black_box_probability']]

print(f"  Features identificadas:")
print(f"      ‚Ä¢ Categ√≥ricas: {len(categorical_cols)}")
print(f"      ‚Ä¢ Num√©ricas: {len(numerical_cols)}")

# Mostrar exemplos de cada tipo
if categorical_cols:
    print(f"      ‚Ä¢ Exemplos categ√≥ricas: {categorical_cols[:3]}...")
if numerical_cols:
    print(f"      ‚Ä¢ Exemplos num√©ricas: {numerical_cols[:3]}...")

# Verificar se temos features suficientes
if len(categorical_cols) + len(numerical_cols) == 0:
    print("  Nenhuma feature encontrada! Criando features b√°sicas...")
    # Criar algumas features b√°sicas se necess√°rio
    df_large['text_length'] = np.random.randint(5, 1000, len(df_large))
    df_large['toxicity_score'] = np.random.beta(2, 5, len(df_large)) * 10
    numerical_cols = ['text_length', 'toxicity_score']
    categorical_cols = ['risk_category', 'content_type']

# Preparar dados para encoding
print("\n Preparando dados para encoding...")
label_encoders = {}
X_encoded = df_large[numerical_cols].copy()  # Come√ßar com features num√©ricas

# Codificar vari√°veis categ√≥ricas
print("  Codificando features categ√≥ricas...")
successful_encodings = 0
for col in categorical_cols:
    try:
        if col in df_large.columns and df_large[col].notna().any():
            le = LabelEncoder()
            # Garantir que n√£o h√° NaN
            clean_values = df_large[col].fillna('Missing').astype(str)
            X_encoded[col] = le.fit_transform(clean_values)
            label_encoders[col] = le
            successful_encodings += 1
            if successful_encodings <= 3:  # Mostrar apenas os primeiros
                print(f"      {col} codificada ({len(le.classes_)} categorias)")
        else:
            print(f"      {col} n√£o dispon√≠vel para encoding")
    except Exception as e:
        print(f"      Erro ao codificar {col}: {str(e)[:50]}...")
print(f"  Total de features codificadas: {successful_encodings}")

# Definir targets
y = df_large['black_box_target']
y_proba = df_large['black_box_probability']

print(f"\n Targets definidos:")
print(f"      ‚Ä¢ y shape: {y.shape}")
print(f"      ‚Ä¢ y_proba shape: {y_proba.shape}")
print(f"      ‚Ä¢ Distribui√ß√£o de y: {dict(y.value_counts())}")

# Split treino/teste (80/20)
print("\n Dividindo em treino e teste...")
X_train, X_test, y_train, y_test, y_proba_train, y_proba_test = train_test_split(
    X_encoded, y, y_proba, test_size=0.2, random_state=42, stratify=y
)

print(f" Divis√£o conclu√≠da:")
print(f"      ‚Ä¢ Conjunto de treino: {X_train.shape[0]:,} amostras, {X_train.shape[1]} features")
print(f"      ‚Ä¢ Conjunto de teste: {X_test.shape[0]:,} amostras, {X_test.shape[1]} features")
print(f"      ‚Ä¢ Distribui√ß√£o treino: {dict(y_train.value_counts())}")
print(f"      ‚Ä¢ Distribui√ß√£o teste: {dict(y_test.value_counts())}")

# Verifica√ß√£o final da qualidade dos dados
print("\n Verifica√ß√£o final de qualidade:")
print(f"      ‚Ä¢ Valores NaN em X_train: {X_train.isnull().sum().sum()}")
print(f"      ‚Ä¢ Valores NaN em X_test: {X_test.isnull().sum().sum()}")
print(f"      ‚Ä¢ Valores NaN em y_train: {y_train.isnull().sum()}")
print(f"      ‚Ä¢ Valores NaN em y_test: {y_test.isnull().sum()}")

# Se houver NaN, preencher
if X_train.isnull().sum().sum() > 0:
    print("   üõ†Ô∏è  Preenchendo valores NaN...")
    X_train = X_train.fillna(X_train.mean())
    X_test = X_test.fillna(X_test.mean())

print("    PR√â-PROCESSAMENTO CONCLU√çDO COM SUCESSO!")

# MOSTRAR RESUMO ESTAT√çSTICO
print("\n" + "="*60)
print("RESUMO ESTAT√çSTICO - DATASET PR√â-PROCESSADO")
print("="*60)

print(f" DATASET FINAL:")
print(f"   ‚Ä¢ Total de amostras: {X_encoded.shape[0]:,}")
print(f"   ‚Ä¢ Total de features: {X_encoded.shape[1]}")
print(f"   ‚Ä¢ Features num√©ricas: {len(numerical_cols)}")
print(f"   ‚Ä¢ Features categ√≥ricas codificadas: {len(categorical_cols)}")
print(f"   ‚Ä¢ Mem√≥ria: {X_encoded.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

print(f"\n DISTRIBUI√á√ÉO DO TARGET:")
target_counts = y.value_counts()
total = len(y)
print(f"   ‚Ä¢ Classe 0 (Safe): {target_counts[0]:,} ({target_counts[0]/total*100:.1f}%)")
print(f"   ‚Ä¢ Classe 1 (Harmful): {target_counts[1]:,} ({target_counts[1]/total*100:.1f}%)")

print(f"\n ESTAT√çSTICAS DAS FEATURES NUM√âRICAS:")
if len(numerical_cols) > 0:
    numeric_stats = df_large[numerical_cols].describe()
    print(f"   ‚Ä¢ M√©dia de toxicity_score: {numeric_stats.loc['mean', 'toxicity_score']:.2f}")
    print(f"   ‚Ä¢ M√©dia de sensitivity_score: {numeric_stats.loc['mean', 'sensitivity_score']:.2f}")
else:
    print("   ‚Ä¢ Nenhuma feature num√©rica dispon√≠vel")

print(f"\nESTAT√çSTICAS DAS FEATURES CATEG√ìRICAS:")
if len(categorical_cols) > 0:
    for col in categorical_cols[:2]:  # Mostrar apenas as 2 primeiras
        value_counts = df_large[col].value_counts()
        print(f"   ‚Ä¢ {col}: {len(value_counts)} categorias √∫nicas")
else:
    print("   ‚Ä¢ Nenhuma feature categ√≥rica dispon√≠vel")

print("\n" + "="*60)
print("PRONTO PARA TREINAMENTO DOS MODELOS!")
print("="*60)

# 2. AN√ÅLISE EXPLORAT√ìRIA E SEGMENTA√á√ÉO PARA DATASET ESCAL√ÅVEL
print("\n" + "="*60)
print("2. AN√ÅLISE EXPLORAT√ìRIA E SEGMENTA√á√ÉO DO DATASET ESCAL√ÅVEL")
print("="*60)

# Usar as features j√° identificadas no pr√©-processamento
print(f"   Estrutura do dataset:")
print(f"      ‚Ä¢ Total de features: {X_encoded.shape[1]}")
print(f"      ‚Ä¢ Features categ√≥ricas originais: {len(categorical_cols)}")
print(f"      ‚Ä¢ Features num√©ricas originais: {len(numerical_cols)}")
print(f"      ‚Ä¢ Features ap√≥s encoding: {X_encoded.shape[1]}")

# Mostrar alguas features importantes
print(f"\n   Exemplos de features dispon√≠veis:")
print(f"      ‚Ä¢ Categ√≥ricas (primeiras 5): {categorical_cols[:5]}")
print(f"      ‚Ä¢ Num√©ricas (primeiras 5): {numerical_cols[:5]}")

# 3. ESTAT√çSTICAS DO DATASET
print("\n3. ESTAT√çSTICAS DESCRITIVAS")

print("\n--- Features Num√©ricas ---")
print(df[numerical_features].describe())

print("\n--- Features Categ√≥ricas ---")
for feature in categorical_features:
    print(f"\n{feature}:")
    print(df[feature].value_counts())

# 3. ESTAT√çSTICAS DO DATASET ESCAL√ÅVEL
print("\n" + "="*60)
print("3. ESTAT√çSTICAS DESCRITIVAS - DATASET ESCAL√ÅVEL")
print("="*60)

print("\n--- Estat√≠sticas das Features Num√©ricas Principais ---")
if len(numerical_cols) > 0:
    # Mostrar apenas algumas num√©ricas principais para n√£o sobrecarregar
    main_numerical = ['toxicity_score', 'sensitivity_score', 'text_length'] + numerical_cols[:2]
    available_numerical = [col for col in main_numerical if col in df_large.columns]

    if available_numerical:
        print(df_large[available_numerical].describe())
    else:
        # Se n√£o tiver as espec√≠ficas, pegar as primeiras num√©ricas dispon√≠veis
        if len(numerical_cols) > 0:
            print(df_large[numerical_cols[:5]].describe())
else:
    print("   Nenhuma feature num√©rica dispon√≠vel")

print("\n--- Estat√≠sticas das Features Categ√≥ricas Principais ---")
if len(categorical_cols) > 0:
    for feature in categorical_cols[:3]:  # Mostrar apenas as 3 principais
        print(f"\n{feature}:")
        value_counts = df_large[feature].value_counts()
        print(f"   Total categorias: {len(value_counts)}")
        print(f"   Top 5 valores:")
        for i, (val, count) in enumerate(value_counts.head().items()):
            print(f"      {i+1}. {val}: {count} ({count/len(df_large)*100:.1f}%)")
else:
    print("   Nenhuma feature categ√≥rica dispon√≠vel")

# 4. VISUALIZA√á√ïES PARA DATASET ESCAL√ÅVEL
print("\n" + "="*60)
print("4. VISUALIZA√á√ïES DO DATASET ESCAL√ÅVEL")
print("="*60)

print("   Gerando visualiza√ß√µes adaptadas para dataset grande...")

fig, axes = plt.subplots(2, 3, figsize=(20, 12))
fig.suptitle('AN√ÅLISE EXPLORAT√ìRIA - DATASET WILDJAILBREAK ESCAL√ÅVEL', fontsize=16, fontweight='bold')

# 4.1 Distribui√ß√£o do target
axes[0,0].pie(df_large['black_box_target'].value_counts(),
              labels=['Safe', 'Harmful'],
              autopct='%1.1f%%',
              colors=['lightgreen', 'lightcoral'],
              startangle=90)
axes[0,0].set_title('Distribui√ß√£o do Target\n(Caixa-Preta)', fontweight='bold')

# 4.2 Distribui√ß√£o das categorias de risco (top 10)
if 'risk_category' in df_large.columns:
    risk_counts = df_large['risk_category'].value_counts().head(10)
    axes[0,1].bar(range(len(risk_counts)), risk_counts.values, color='skyblue')
    axes[0,1].set_title('Top 10 Categorias de Risco', fontweight='bold')
    axes[0,1].set_xticks(range(len(risk_counts)))
    axes[0,1].set_xticklabels(risk_counts.index, rotation=45, ha='right')
    axes[0,1].set_ylabel('Frequ√™ncia')
else:
    axes[0,1].text(0.5, 0.5, 'risk_category\nn√£o dispon√≠vel',
                   ha='center', va='center', transform=axes[0,1].transAxes)
    axes[0,1].set_title('Categorias de Risco', fontweight='bold')

# 4.3 Distribui√ß√£o de toxicity score
if 'toxicity_score' in df_large.columns:
    sns.histplot(data=df_large, x='toxicity_score', ax=axes[0,2], kde=True, color='purple')
    axes[0,2].set_title('Distribui√ß√£o do Toxicity Score', fontweight='bold')
    axes[0,2].set_xlabel('Toxicity Score')
    axes[0,2].set_ylabel('Densidade')
else:
    axes[0,2].text(0.5, 0.5, 'toxicity_score\nn√£o dispon√≠vel',
                   ha='center', va='center', transform=axes[0,2].transAxes)
    axes[0,2].set_title('Toxicity Score', fontweight='bold')

# 4.4 Toxicity score por target
if 'toxicity_score' in df_large.columns:
    sns.boxplot(data=df_large, x='black_box_target', y='toxicity_score', ax=axes[1,0], palette=['lightgreen', 'lightcoral'])
    axes[1,0].set_title('Toxicity Score vs Target', fontweight='bold')
    axes[1,0].set_xticklabels(['Safe', 'Harmful'])
    axes[1,0].set_xlabel('Target')
    axes[1,0].set_ylabel('Toxicity Score')
else:
    axes[1,0].text(0.5, 0.5, 'Dados n√£o dispon√≠veis',
                   ha='center', va='center', transform=axes[1,0].transAxes)
    axes[1,0].set_title('Toxicity vs Target', fontweight='bold')

# 4.5 Distribui√ß√£o de sensitivity score por target
if 'sensitivity_score' in df_large.columns:
    sns.histplot(data=df_large, x='sensitivity_score', hue='black_box_target',
                 ax=axes[1,1], alpha=0.6, palette=['lightgreen', 'lightcoral'])
    axes[1,1].set_title('Sensitivity Score por Target', fontweight='bold')
    axes[1,1].set_xlabel('Sensitivity Score')
    axes[1,1].set_ylabel('Frequ√™ncia')
else:
    axes[1,1].text(0.5, 0.5, 'sensitivity_score\nn√£o dispon√≠vel',
                   ha='center', va='center', transform=axes[1,1].transAxes)
    axes[1,1].set_title('Sensitivity Score', fontweight='bold')

# 4.6 Heatmap de correla√ß√£o (apenas features num√©ricas principais)
if len(numerical_cols) > 0:
    # Selecionar at√© 8 features num√©ricas para o heatmap
    corr_features = numerical_cols[:8] + ['black_box_target']
    corr_features = [f for f in corr_features if f in df_large.columns]

    if len(corr_features) > 1:
        correlation_matrix = df_large[corr_features].corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                   ax=axes[1,2], fmt='.2f', cbar_kws={'shrink': 0.8})
        axes[1,2].set_title('Matriz de Correla√ß√£o\n(Features Principais)', fontweight='bold')
    else:
        axes[1,2].text(0.5, 0.5, 'Poucas features\npara correla√ß√£o',
                       ha='center', va='center', transform=axes[1,2].transAxes)
        axes[1,2].set_title('Matriz de Correla√ß√£o', fontweight='bold')
else:
    axes[1,2].text(0.5, 0.5, 'Sem features num√©ricas\npara correla√ß√£o',
                   ha='center', va='center', transform=axes[1,2].transAxes)
    axes[1,2].set_title('Matriz de Correla√ß√£o', fontweight='bold')

plt.tight_layout()
plt.show()

# 5. MODELO CAIXA-PRETA PARA DATASET ESCAL√ÅVEL
print("\n" + "="*60)
print("5. TREINANDO MODELO CAIXA-PRETA (Random Forest - Dataset Escal√°vel)")
print("="*60)

print("    Preparando dados para treinamento...")
print(f"      ‚Ä¢ X_train shape: {X_train.shape}")
print(f"      ‚Ä¢ X_test shape: {X_test.shape}")
print(f"      ‚Ä¢ y_train distribui√ß√£o: {dict(y_train.value_counts())}")
print(f"      ‚Ä¢ y_test distribui√ß√£o: {dict(y_test.value_counts())}")

print("    Treinando Random Forest...")
black_box_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=15,
    min_samples_split=20,
    min_samples_leaf=10,
    max_features='sqrt',
    bootstrap=True,
    n_jobs=-1,
    random_state=42,
    verbose=1
)

black_box_model.fit(X_train, y_train)

# Previs√µes do modelo caixa-preta
print("   Gerando previs√µes...")
y_black_box_pred = black_box_model.predict(X_test)
y_black_box_proba = black_box_model.predict_proba(X_test)[:, 1]

# Estat√≠sticas do modelo caixa-preta (SEM accuracy_score)
black_box_f1 = f1_score(y_test, y_black_box_pred)
black_box_accuracy = np.mean(y_black_box_pred == y_test)  # Acur√°cia manual
black_box_auc = roc_auc_score(y_test, y_black_box_proba)

# Calcular precision e recall manualmente se necess√°rio
from sklearn.metrics import precision_score, recall_score
black_box_precision = precision_score(y_test, y_black_box_pred)
black_box_recall = recall_score(y_test, y_black_box_pred)

print(f"   Modelo Caixa-Preta treinado com sucesso!")
print(f"      F1 Score: {black_box_f1:.4f}")
print(f"      Acur√°cia: {black_box_accuracy:.4f}")
print(f"      AUC-ROC: {black_box_auc:.4f}")
print(f"      Precision: {black_box_precision:.4f}")
print(f"      Recall: {black_box_recall:.4f}")

# Mostrar impot√¢ncia das features
print(f"\n   Top 10 Features Mais Importantes:")
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': black_box_model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in feature_importance.head(10).iterrows():
    print(f"      {i+1:2d}. {row['feature']}: {row['importance']:.4f}")

# 6. MODELO SURROGATE PARA DATASET ESCAL√ÅVEL
print("\n" + "="*60)
print("6. TREINANDO MODELO SURROGATE (Dataset Escal√°vel)")
print("="*60)

# CORRE√á√ÉO: Usar as features mais importantes para o surrogate
print("   Selecionando features mais importantes para o surrogate...")
feature_importance = black_box_model.feature_importances_
important_features_idx = np.argsort(feature_importance)[-15:]  # Top 15 features

# Obter nomes das features importantes
feature_names = X_train.columns.tolist()
surrogate_features = [feature_names[i] for i in important_features_idx]

print(f"    Selecionadas {len(surrogate_features)} features mais importantes")
print(f"     ‚Ä¢ Top 5 features: {surrogate_features[-5:]}")

# Preparar dados para surrogate
X_train_surrogate = X_train.iloc[:, important_features_idx]
X_test_surrogate = X_test.iloc[:, important_features_idx]

# CORRE√á√ÉO: Precisamos das previs√µes da caixa-preta no CONJUNTO DE TREINO tamb√©m
print("    Gerando previs√µes da caixa-preta no conjunto de treino...")
y_black_box_pred_train = black_box_model.predict(X_train)
y_black_box_proba_train = black_box_model.predict_proba(X_train)[:, 1]

# OP√á√ÉO 1: Surrogate Regressor (imita probabilidades)
print("    Treinando Surrogate Regressor...")
surrogate_regressor = DecisionTreeRegressor(
    max_depth=6,  # Profundidade um pouco maior para dataset complexo
    min_samples_leaf=50,
    min_samples_split=100,
    random_state=42
)

# CORRE√á√ÉO: Usar probabilidades do TREINO
surrogate_regressor.fit(X_train_surrogate, y_black_box_proba_train)

# Previs√µes do surrogate regressor
y_surrogate_proba_reg = surrogate_regressor.predict(X_test_surrogate)
y_surrogate_pred_reg = (y_surrogate_proba_reg > 0.5).astype(int)

# OP√á√ÉO 2: Surrogate Classifier (imita classifica√ß√µes)
print("    Treinando Surrogate Classifier...")
surrogate_classifier = DecisionTreeClassifier(
    max_depth=6,
    min_samples_leaf=50,
    min_samples_split=100,
    random_state=42
)

# CORRE√á√ÉO: Usar classifica√ß√µes do TREINO
surrogate_classifier.fit(X_train_surrogate, y_black_box_pred_train)

y_surrogate_pred_clf = surrogate_classifier.predict(X_test_surrogate)
y_surrogate_proba_clf = surrogate_classifier.predict_proba(X_test_surrogate)[:, 1]

# Comparar ambas as abordagens
print("  Calculando m√©tricas de compara√ß√£o...")
surrogate_f1_reg = f1_score(y_test, y_surrogate_pred_reg)
surrogate_f1_clf = f1_score(y_test, y_surrogate_pred_clf)

fidelity_reg = np.mean(y_surrogate_pred_reg == y_black_box_pred)
fidelity_clf = np.mean(y_surrogate_pred_clf == y_black_box_pred)

surrogate_auc_reg = roc_auc_score(y_test, y_surrogate_proba_reg)
surrogate_auc_clf = roc_auc_score(y_test, y_surrogate_proba_clf)

print(f"\n   COMPARA√á√ÉO DAS ABORDAGENS:")
print(f"   Regressor (probabilidades):")
print(f"      ‚Ä¢ F1 Score: {surrogate_f1_reg:.4f}")
print(f"      ‚Ä¢ AUC-ROC: {surrogate_auc_reg:.4f}")
print(f"      ‚Ä¢ Fidelidade: {fidelity_reg:.4f}")
print(f"      ‚Ä¢ Correla√ß√£o: {np.corrcoef(y_black_box_proba, y_surrogate_proba_reg)[0,1]:.4f}")

print(f"   Classifier (decis√µes):")
print(f"      ‚Ä¢ F1 Score: {surrogate_f1_clf:.4f}")
print(f"      ‚Ä¢ AUC-ROC: {surrogate_auc_clf:.4f}")
print(f"      ‚Ä¢ Fidelidade: {fidelity_clf:.4f}")

# Escolher a melhor abordagem baseado na fidelidade
if fidelity_reg >= fidelity_clf:
    surrogate_model = surrogate_regressor
    y_surrogate_final_proba = y_surrogate_proba_reg
    y_surrogate_final_pred = y_surrogate_pred_reg
    approach = "Regressor"
    model_type = "DecisionTreeRegressor"
    print(f"\n  Abordagem selecionada: {approach} (probabilidades)")
else:
    surrogate_model = surrogate_classifier
    y_surrogate_final_proba = y_surrogate_proba_clf
    y_surrogate_final_pred = y_surrogate_pred_clf
    approach = "Classifier"
    model_type = "DecisionTreeClassifier"
    print(f"\n  Abordagem selecionada: {approach} (classifica√ß√µes)")

# Estat√≠sticas finais do surrogate
surrogate_f1 = f1_score(y_test, y_surrogate_final_pred)
surrogate_auc = roc_auc_score(y_test, y_surrogate_final_proba)
surrogate_accuracy = np.mean(y_surrogate_final_pred == y_test)  # Usando c√°lculo manual
fidelity = np.mean(y_surrogate_final_pred == y_black_box_pred)

print(f"\n   üéØ M√âTRICAS FINAIS DO SURROGATE:")
print(f"      ‚Ä¢ F1 Score: {surrogate_f1:.4f}")
print(f"      ‚Ä¢ AUC-ROC: {surrogate_auc:.4f}")
print(f"      ‚Ä¢ Acur√°cia: {surrogate_accuracy:.4f}")
print(f"      ‚Ä¢ Fidelidade: {fidelity:.4f} ({fidelity*100:.1f}% de acordo)")

if approach == "Regressor":
    correlation = np.corrcoef(y_black_box_proba, y_surrogate_final_proba)[0,1]
    print(f"      ‚Ä¢ Correla√ß√£o de probabilidades: {correlation:.4f}")

print(f"      ‚Ä¢ Tipo do modelo: {model_type}")
print(f"      ‚Ä¢ Features utilizadas: {len(surrogate_features)}")

# VISUALIZA√á√ÉO DA √ÅRVORE DO SURROGATE (se for classificador)
if approach == "Classifier":
    print(f"\n   Visualizando √°rvore de decis√£o do surrogate...")
    plt.figure(figsize=(20, 10))
    plot_tree(surrogate_classifier,
              feature_names=surrogate_features,
              class_names=['Safe', 'Harmful'],
              filled=True,
              rounded=True,
              fontsize=10,
              max_depth=3)
    plt.title(f'√Årvore de Decis√£o do Surrogate ({approach})', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()
else:
    print(f"\n   Para surrogate regressor, as regras s√£o cont√≠nuas (valores de threshold)")

print("\n" + "="*60)
print(" ETAPAS 2-6 CONCLU√çDAS COM SUCESSO!")
print("="*60)

